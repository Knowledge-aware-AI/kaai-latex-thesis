\addchap*{Abstract}
LLMs have majorly advanced NLP and AI, and next to their ability to perform a wide range of procedural tasks, a major success factor is their internalized factual knowledge. Since \cite{petroni-etal-2019-language}, analyzing this knowledge has gained attention. However, most approaches investigate one question at a time via modest-sized pre-defined samples, introducing an \textit{``availability bias''} \cite{kahnemann} that prevents the discovery of knowledge (or beliefs) of LLMs beyond the experimenter's predisposition.

To address this challenge, we propose a novel methodology to comprehensively materialize an LLM's factual knowledge through recursive querying and result consolidation.

As a prototype, we employ GPT-4o-mini to construct \ourkb, a large-scale knowledge base (KB) comprising 101 million triples for over 2.9 million entities - achieved at 1\% of the cost of previous KB projects. This work marks a milestone in two areas: For LLM research, for the first time, it provides \textit{constructive} insights into the scope and structure of LLMs' knowledge (or beliefs). For KB construction, it pioneers \textit{new pathways} for the long-standing challenge of general-domain KB construction. GPTKB is accessible at \website.